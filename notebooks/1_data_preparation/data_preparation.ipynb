{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "20203c0f",
   "metadata": {},
   "source": [
    "# I. Project Team Members"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0aa3d7f4",
   "metadata": {},
   "source": [
    "| Prepared by | Email | Prepared for |\n",
    "| :-: | :-: | :-: |\n",
    "| **Hardefa Rogonondo** | hardefarogonondo@gmail.com | **Car Detection and Retrieval Engine** |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b05cd469",
   "metadata": {},
   "source": [
    "# II. Notebook Target Definition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47bae1d2",
   "metadata": {},
   "source": [
    "This notebook serves as the data preparation module for the Car Detection and Retrieval Engine project. In this notebook, we will download the dataset required for this project from Roboflow using the provided API key. The retrieved data will be utilized to train our object detection and classification models, forming the foundation for building an efficient car detection and retrieval system."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3213f42d",
   "metadata": {},
   "source": [
    "# III. Notebook Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb5c3810",
   "metadata": {},
   "source": [
    "## III.A. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac84c896",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from roboflow import Roboflow\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "load_dotenv('../../.env')\n",
    "api_key = os.getenv('ROBOFLOW_API_KEY')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7291e85b",
   "metadata": {},
   "source": [
    "## III.B. Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "115e50e7-4f27-4eba-a6ba-6ec7bfe3ce14",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = Roboflow(api_key=api_key)\n",
    "temp_dir = './data/'\n",
    "obj_detection_dir = '../../data/object_detection'\n",
    "obj_classification_dir = '../../data/object_classification'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9d6314-3357-4e3f-8394-013f1af02d07",
   "metadata": {},
   "source": [
    "### III.B.1. Object Detection Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c54961d-d622-4135-9e01-3cc98c758c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in ./data/ to voc:: 100%|███████████████████| 594959/594959 [00:38<00:00, 15644.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Dataset Version Zip to ./data/ in voc:: 100%|███████████████████████| 18425/18425 [00:11<00:00, 1611.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded to: D:\\Projects\\Software\\car-detection-and-retrieval-engine\\notebooks\\1_data_preparation\\data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "obj_detection_project = rf.workspace(\"lynkeus03\").project(\"vehicle-detection-by9xs\")\n",
    "obj_detection_version = obj_detection_project.version(3)\n",
    "obj_detection_dataset = obj_detection_version.download(model_format='voc', location=temp_dir)\n",
    "print(f\"Dataset downloaded to: {obj_detection_dataset.location}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4723b53-00a9-42ac-877d-52f709b13ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating final destination directories...\n",
      "All final destination directories created.\n"
     ]
    }
   ],
   "source": [
    "all_images_path = os.path.join(obj_detection_dataset.location, \"train\")\n",
    "all_labels_path = os.path.join(obj_detection_dataset.location, \"train\")\n",
    "final_train_images_path = os.path.join(obj_detection_dir, \"train\", \"images\")\n",
    "final_train_labels_path = os.path.join(obj_detection_dir, \"train\", \"labels\")\n",
    "final_test_images_path = os.path.join(obj_detection_dir, \"test\", \"images\")\n",
    "final_test_labels_path = os.path.join(obj_detection_dir, \"test\", \"labels\")\n",
    "final_valid_images_path = os.path.join(obj_detection_dir, \"valid\", \"images\")\n",
    "final_valid_labels_path = os.path.join(obj_detection_dir, \"valid\", \"labels\")\n",
    "print(\"\\nCreating final destination directories...\")\n",
    "os.makedirs(final_train_images_path, exist_ok=True)\n",
    "os.makedirs(final_train_labels_path, exist_ok=True)\n",
    "os.makedirs(final_test_images_path, exist_ok=True)\n",
    "os.makedirs(final_test_labels_path, exist_ok=True)\n",
    "os.makedirs(final_valid_images_path, exist_ok=True)\n",
    "os.makedirs(final_valid_labels_path, exist_ok=True)\n",
    "print(\"All final destination directories created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cf7fe4e-bfa1-4633-bbeb-a47ec5600b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 9211 complete image-annotation pairs in the 'train' dataset.\n",
      "Splitting into: Train=7368, Valid=922, Test=921 samples.\n",
      "\n",
      "Moving 7368 files to train split in final destination...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 7368/7368 [00:06<00:00, 1135.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Moving 921 files to test split in final destination...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 921/921 [00:00<00:00, 1681.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Moving 922 files to valid split in final destination...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 922/922 [00:00<00:00, 1767.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset splitting and moving to final destination complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Manual Train-Test-Split\n",
    "image_files = [files for files in os.listdir(all_images_path) if files.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "label_files = [files for files in os.listdir(all_labels_path) if files.lower().endswith(\".xml\")]\n",
    "image_stems = {os.path.splitext(files)[0] for files in image_files}\n",
    "label_stems = {os.path.splitext(files)[0] for files in label_files}\n",
    "common_stems = list(image_stems.intersection(label_stems))\n",
    "random.shuffle(common_stems)\n",
    "print(f\"\\nFound {len(common_stems)} complete image-annotation pairs in the 'train' dataset.\")\n",
    "\n",
    "train_ratio = 0.8\n",
    "test_ratio = 0.1\n",
    "valid_ratio = 0.1\n",
    "total_samples = len(common_stems)\n",
    "train_size = int(train_ratio * total_samples)\n",
    "test_size = int(test_ratio * total_samples)\n",
    "valid_size = total_samples - train_size - test_size\n",
    "print(f\"Splitting into: Train={train_size}, Valid={valid_size}, Test={test_size} samples.\")\n",
    "\n",
    "train_stems = common_stems[:train_size]\n",
    "valid_stems = common_stems[train_size : train_size + valid_size]\n",
    "test_stems = common_stems[train_size + valid_size :]\n",
    "splits_final_dest = {\n",
    "    \"train\": {\"stems\": train_stems, \"img_path\": final_train_images_path, \"label_path\": final_train_labels_path},\n",
    "    \"test\": {\"stems\": test_stems, \"img_path\": final_test_images_path, \"label_path\": final_test_labels_path},\n",
    "    \"valid\": {\"stems\": valid_stems, \"img_path\": final_valid_images_path, \"label_path\": final_valid_labels_path}\n",
    "}\n",
    "\n",
    "for split_name, data in splits_final_dest.items():\n",
    "    print(f\"\\nMoving {len(data['stems'])} files to {split_name} split in final destination...\")\n",
    "    for stem in tqdm(data[\"stems\"]):\n",
    "        image_src = None\n",
    "        for ext in [\".jpg\", \".jpeg\", \".png\"]:\n",
    "            potential_path = os.path.join(all_images_path, stem + ext)\n",
    "            if os.path.exists(potential_path):\n",
    "                image_src = potential_path\n",
    "                break\n",
    "        label_src = os.path.join(all_labels_path, stem + \".xml\")\n",
    "        if image_src and os.path.exists(label_src):\n",
    "            shutil.move(image_src, os.path.join(data[\"img_path\"], os.path.basename(image_src)))\n",
    "            shutil.move(label_src, os.path.join(data[\"label_path\"], os.path.basename(label_src)))\n",
    "        else:\n",
    "            print(f\"Warning: Missing image or annotation for stem {stem}. Skipping...\")\n",
    "print(\"\\nDataset splitting and moving to final destination complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36179e5a-134f-409e-9289-4ace6a845605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporary download folder ./data/ cleaned up.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    if os.path.exists(temp_dir):\n",
    "        shutil.rmtree(temp_dir)\n",
    "        print(f\"Temporary download folder {temp_dir} cleaned up.\")\n",
    "except OSError as error:\n",
    "    print(f\"Error removing temporary directory {temp_dir}: {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a246bf9f-2550-41f2-a7d7-d383d1869491",
   "metadata": {},
   "source": [
    "### III.B.2. Object Classification Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f425995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in ./data/ to voc:: 100%|███████████████████| 203077/203077 [00:13<00:00, 14906.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Dataset Version Zip to ./data/ in voc:: 100%|█████████████████████████| 2517/2517 [00:01<00:00, 1595.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded to temporary folder: D:\\Projects\\Software\\car-detection-and-retrieval-engine\\notebooks\\1_data_preparation\\data\n"
     ]
    }
   ],
   "source": [
    "obj_classification_project = rf.workspace(\"smartnozzle\").project(\"modelmobil\")\n",
    "obj_classification_version = obj_classification_project.version(27)\n",
    "obj_classification_dataset = obj_classification_version.download(\"voc\", location=temp_dir)\n",
    "print(f\"Dataset downloaded to temporary folder: {obj_classification_dataset.location}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43e00e67-a67a-4f27-a519-b2738ed1b5eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved README.dataset.txt to ../../data/object_classification\n",
      "Moved README.roboflow.txt to ../../data/object_classification\n",
      "Moved test to ../../data/object_classification\n",
      "Moved train to ../../data/object_classification\n",
      "Moved valid to ../../data/object_classification\n"
     ]
    }
   ],
   "source": [
    "for item in os.listdir(temp_dir):\n",
    "    source = os.path.join(temp_dir, item)\n",
    "    destination = os.path.join(obj_classification_dir, item)\n",
    "    shutil.move(source, destination)\n",
    "    print(f\"Moved {item} to {obj_classification_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "364ce865-e350-478e-80e1-d41e26f479f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporary download folder ./data/ cleaned up.\n"
     ]
    }
   ],
   "source": [
    "shutil.rmtree(temp_dir)\n",
    "print(f\"Temporary download folder {temp_dir} cleaned up.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
